{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 01-Load a Dimension table\r\n",
        "This notebook extracts product data from parquet files and loads it into Delta lake and Synapse table.\r\n",
        "This notebook executes some popular transformations you will encounter in real-life scenarios\r\n",
        "\t\t\t\t\t\r\n",
        "## Contents\r\n",
        "1. Extract\r\n",
        "1. Transform\r\n",
        "1. Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set Parameters\r\n",
        "\r\n",
        "# Set path to source files\r\n",
        "basePath = \"abfss://data@REPLACE_DATALAKE_NAME.dfs.core.windows.net/sample/AdventureWorksDW2019/dbo/\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1. Extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Create a spark dataframe with product data\r\n",
        "productDF = spark.read.parquet(basePath + \"DimProduct\")\r\n",
        "productDF.createOrReplaceTempView(\"product_tmp\")\r\n",
        "display(productDF.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Always import these two sets of libraries at a minimum for spark transformations\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Join two dataframes to build a Product Category Table\r\n",
        "productSubCategoryDF = spark.read.parquet(basePath + \"DimProductSubcategory\")\r\n",
        "productCategoryDF = spark.read.parquet(basePath + \"DimProductCategory\")\r\n",
        "\r\n",
        "prodcatDF = productSubCategoryDF.join(productCategoryDF,productSubCategoryDF.ProductCategoryKey == productCategoryDF.ProductCategoryKey,\"inner\")\\\r\n",
        "                                .select(col(\"ProductSubcategoryKey\"),col(\"EnglishProductSubcategoryName\").alias(\"ProductSubCategory\"),col(\"EnglishProductCategoryName\").alias(\"ProductCategory\"))\r\n",
        "\r\n",
        "prodcatDF.createOrReplaceTempView(\"prodCategory_tmp\")\r\n",
        "display(prodcatDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3. Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create temporary table to faciliate data transfer between scala and python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "CREATE OR REPLACE TEMPORARY VIEW prod_tmp AS\r\n",
        "SELECT \r\n",
        "pt.ProductKey as product_sk,\r\n",
        "pt.ProductAlternateKey as product_id,\r\n",
        "pt.EnglishProductName as ProductName,\r\n",
        "pct.ProductSubCategory,\r\n",
        "pct.ProductCategory\r\n",
        "FROM product_tmp pt, prodCategory_tmp pct\r\n",
        "WHERE\r\n",
        "pt.ProductSubcategoryKey = pct.ProductSubcategoryKey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create a delta lake table if doesn't already exist for product data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Creating spark tables using delta format allow ACID transactions on data lake tables\r\n",
        "-- This is a one-time task\r\n",
        "CREATE TABLE IF NOT EXISTS sparklakehouse.stg_product USING DELTA\r\n",
        "AS\r\n",
        "SELECT * FROM prod_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Merge new and changed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "MERGE INTO sparklakehouse.stg_product t\r\n",
        "USING prod_tmp s \r\n",
        "ON t.product_sk = s.product_sk \r\n",
        "WHEN MATCHED THEN UPDATE SET *\r\n",
        "WHEN NOT MATCHED THEN INSERT *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Clean up old versions of the data periodically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "VACUUM sparklakehouse.stg_product RETAIN 168 HOURS;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Finally upload the data into Synapse Analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": false
      },
      "source": [
        "%%spark\r\n",
        "// Create a scala data frame from the Temporary table\r\n",
        "val scala_df = spark.sqlContext.sql (\"select * from prod_tmp\")\r\n",
        "\t\t\t\t\t\r\n",
        "// Create a staging table after which we can run a stored procedure to create the final table\r\n",
        "scala_df.write.synapsesql(\"SQLTestPool.dbo.StgProduct\", Constants.INTERNAL)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}
