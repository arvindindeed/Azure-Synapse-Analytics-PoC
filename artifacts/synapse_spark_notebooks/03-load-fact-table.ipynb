{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 01-Load a Fact table\r\n",
        "This notebook extracts data from various sources and loads into a staging table in Synapse.\r\n",
        "This notebook executes some popular transformations you will encounter in real-life scenarios\r\n",
        "\t\t\t\t\t\r\n",
        "## Contents\r\n",
        "1. Extract\r\n",
        "1. Transform\r\n",
        "1. Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set Parameters\r\n",
        "\r\n",
        "# Set the date when the target table was last loaded\r\n",
        "lastRefreshDate = \"2021-10-09\"\r\n",
        "\r\n",
        "# Set the name of the delta lake table name\r\n",
        "sourceTableName = \"sparklakehouse.stg_internet_sales\"\r\n",
        "\r\n",
        "# Set path to source files\r\n",
        "basePath = \"abfss://data@REPLACE_DATALAKE_NAME.dfs.core.windows.net/sample/AdventureWorksDW2019/dbo/\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1. Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "a. Extract from Delta Lake table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Extract from Delta lake staging table\r\n",
        "\r\n",
        "newDF = spark.sql(\"SELECT * FROM \"+sourceTableName)\r\n",
        "\r\n",
        "display(newDF.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "newDF.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check how many partitions the data is distributed into\r\n",
        "newDF.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Extract only changes since the last refresh date\r\n",
        "This is a brute force way of figuring out the changes. \r\n",
        "\r\n",
        "Best way would be to get only the changed data from the source system.\r\n",
        "\r\n",
        "The below code is to demonstrate leveraging Delta table history details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get table details\r\n",
        "tblHistDF = spark.sql(\"DESCRIBE HISTORY \"+sourceTableName)\r\n",
        "tblDetDF = spark.sql(\"DESCRIBE DETAIL \"+sourceTableName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get the version number from which to extract changes from source delta table\r\n",
        "if tblHistDF.filter(\"timestamp <= '\"+lastRefreshDate+\" 00:00:00'\").count()==0:\r\n",
        "    # if there are no history records before last refresh date then it is an initial load\r\n",
        "    # We set the prevVersion to -1 to indicate it's an initial load\r\n",
        "    prevVersion = -1\r\n",
        "else:\r\n",
        "    prevVersion = tblHistDF.filter(\"timestamp <= '\"+lastRefreshDate+\" 00:00:00'\").agg({'version': 'max'}).collect()[0][0]\r\n",
        "currVersion = tblHistDF.filter(\"timestamp >= '\"+lastRefreshDate+\" 00:00:00'\").agg({'version': 'max'}).collect()[0][0]\r\n",
        "\r\n",
        "print(\"The latest version before last refresh date is \"+str(prevVersion))\r\n",
        "print(\"The latest version after last refresh date is \"+str(currVersion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# If this is an initial load get the latest data\r\n",
        "if prevVersion == -1:\r\n",
        "    newDF = spark.sql(\"SELECT * FROM \"+sourceTableName)\r\n",
        "# If its not an initial load then use minus to find the changed rows\r\n",
        "# This process will get easier with future versions of delta.io project\r\n",
        "else:\r\n",
        "    prevDF = spark.read.format(\"delta\").option(\"versionAsOf\", prevVersion).load(tablePath)\r\n",
        "    currDF = spark.read.format(\"delta\").option(\"versionAsOf\", currVersion).load(tablePath)\r\n",
        "    prevDF.createOrReplaceTempView(\"prev_tmp\")\r\n",
        "    currDF.createOrReplaceTempView(\"curr_tmp\")\r\n",
        "    newDF = spark.sql(\"SELECT * FROM curr_tmp MINUS SELECT * FROM prev_tmp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "newDF.createOrReplaceTempView(\"new_data_tmp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "###### Extract Other Sources\r\n",
        "Extract from Parquet file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Create a spark dataframe with raw data\r\n",
        "customerDF = spark.read.parquet(basePath + \"DimCustomer\")\r\n",
        "customerDF.createOrReplaceTempView(\"customer_tmp\")\r\n",
        "display(customerDF.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Create a spark dataframe with raw data\r\n",
        "geographyDF = spark.read.parquet(basePath + \"DimGeography\")\r\n",
        "geographyDF.createOrReplaceTempView(\"geography_tmp\")\r\n",
        "display(geographyDF.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Extract from JSON file from Web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "import requests\r\n",
        "from notebookutils import mssparkutils\r\n",
        "\r\n",
        "url = \"https://raw.githubusercontent.com/samayo/country-json/master/src/country-by-population.json\"\r\n",
        "r = requests.get(url, allow_redirects=True)\r\n",
        "\r\n",
        "# Use Microsoft Spark Utilities to read the file and write to file path.\r\n",
        "mssparkutils.fs.put(basePath+\"country-by-population.json\", r.text, True)\r\n",
        "\r\n",
        "# Use the multiline option when the data in the json file is enclosed in []\r\n",
        "countriesDF = spark.read.option(\"multiline\",\"true\").json(basePath+\"country-by-population.json\")\r\n",
        "countriesDF.createOrReplaceTempView(\"countries_tmp\")\r\n",
        "display(countriesDF.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "###### Some important performance settings to be adjusted and experimented with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check how many partitions will be used to shuffle data between executors\r\n",
        "# As a rule of thumb, each partition shouldn't be larger than 128MB\r\n",
        "spark.conf.get(\"spark.sql.shuffle.partitions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check how many cores will be used\r\n",
        "print(spark.sparkContext.defaultParallelism)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set this to a high number just to be safe for large data sets\r\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check if AQE is enable. \r\n",
        "spark.conf.get(\"spark.sql.adaptive.enabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Enable AQE to ensure Shuffle partition number gets set automatically depending on the data set\r\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Always import these two sets of libraries at a minimum for spark transformations\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "describe new_data_tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT \r\n",
        "ndt.order_date_sk,\r\n",
        "ndt.order_date,\r\n",
        "ndt.product_sk,\r\n",
        "ndt.customer_sk,\r\n",
        "ndt.ship_date_sk,\r\n",
        "ndt.due_date_sk,\r\n",
        "ndt.promotion_sk,\r\n",
        "ndt.currency_sk,\r\n",
        "ndt.sales_territory_sk,\r\n",
        "ndt.sales_order_no,\r\n",
        "ndt.sales_order_line_no,\r\n",
        "ndt.revision_no,\r\n",
        "cnt.country,\r\n",
        "cnt.population,\r\n",
        "ndt.order_qty as quantity,\r\n",
        "ndt.sales_amount as sales,\r\n",
        "ndt.discount_amount as discount\r\n",
        "FROM \r\n",
        "new_data_tmp ndt, \r\n",
        "customer_tmp ct, \r\n",
        "geography_tmp gt,\r\n",
        "countries_tmp cnt\r\n",
        "WHERE \r\n",
        "ndt.customer_sk = ct.CustomerKey \r\n",
        "AND ct.GeographyKey = gt.GeographyKey\r\n",
        "AND gt.EnglishCountryRegionName = cnt.Country\r\n",
        "LIMIT 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "CREATE OR REPLACE TEMPORARY VIEW stg_sales_by_country AS\r\n",
        "SELECT \r\n",
        "ndt.order_date_sk,\r\n",
        "ndt.order_date,\r\n",
        "ndt.product_sk,\r\n",
        "ndt.customer_sk,\r\n",
        "ndt.ship_date_sk,\r\n",
        "ndt.due_date_sk,\r\n",
        "ndt.promotion_sk,\r\n",
        "ndt.currency_sk,\r\n",
        "ndt.sales_territory_sk,\r\n",
        "ndt.sales_order_no,\r\n",
        "ndt.sales_order_line_no,\r\n",
        "ndt.revision_no,\r\n",
        "cnt.country,\r\n",
        "cnt.population,\r\n",
        "ndt.order_qty as quantity,\r\n",
        "ndt.sales_amount as sales,\r\n",
        "ndt.discount_amount as discount\r\n",
        "FROM \r\n",
        "new_data_tmp ndt, \r\n",
        "customer_tmp ct, \r\n",
        "geography_tmp gt,\r\n",
        "countries_tmp cnt\r\n",
        "WHERE \r\n",
        "ndt.customer_sk = ct.CustomerKey \r\n",
        "AND ct.GeographyKey = gt.GeographyKey\r\n",
        "AND gt.EnglishCountryRegionName = cnt.Country"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3. Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "DROP TABLE IF EXISTS sparklakehouse.stg_sales_by_country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "--First Load data into a Delta Lake table\r\n",
        "CREATE TABLE sparklakehouse.stg_sales_by_country USING DELTA AS\r\n",
        "SELECT * FROM stg_sales_by_country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# -- %%sql\r\n",
        "# -- -- We can merge only changes into the target table\r\n",
        "# -- -- This cell should be commented out during the initial load\r\n",
        "# -- -- MERGE INTO sparklakehouse.stg_sales_by_country t\r\n",
        "# -- -- USING stg_sales_by_country s \r\n",
        "# -- -- ON t.country = s.country\r\n",
        "# -- -- WHEN MATCHED THEN UPDATE SET *\r\n",
        "# -- -- WHEN NOT MATCHED THEN INSERT *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# -- %%sql\r\n",
        "# -- -- VACUUM sparklakehouse.stg_sales_by_country RETAIN 168 HOURS;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "// Create a scala data frame from the Temporary table\r\n",
        "val scala_df = spark.sqlContext.sql (\"select * from stg_sales_by_country\")\r\n",
        "\t\t\t\t\t\r\n",
        "// Create a new staging table in Synapse from which we will upload to the final table using a stored procedure executed after this notebook is run\r\n",
        "scala_df.write.synapsesql(\"SQLTestPool.dbo.StgFactSalesByCountry\", Constants.INTERNAL)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}
