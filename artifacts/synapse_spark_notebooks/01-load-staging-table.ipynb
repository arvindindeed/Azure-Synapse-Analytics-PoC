{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 01-Load a staging table\r\n",
        "This notebook extracts data from source system files into a staging table.\r\n",
        "In this example we will extract raw sales data stored in parquet files into a delta lake table\r\n",
        "This notebook executes some popular transformations you will encounter in real-life scenarios\r\n",
        "\t\t\t\t\t\r\n",
        "## Contents\r\n",
        "1. Extract\r\n",
        "1. Transform\r\n",
        "1. Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set Parameters\r\n",
        "\r\n",
        "# Set path to source files\r\n",
        "basePath = \"abfss://synapse@oneclickpocadls.dfs.core.windows.net/AdventureWorksDW2019/dbo/\"\r\n",
        "filePath = \"FactInternetSales\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1. Extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Create a spark dataframe with raw data\r\n",
        "rawDF = spark.read.parquet(basePath + filePath)\r\n",
        "\r\n",
        "# If the source file format is csv or json\r\n",
        "# df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(filePath)\r\n",
        "# df = spark.read.json(filePath)\r\n",
        "\r\n",
        "display(rawDF.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run some exploratory data analysis\r\n",
        "rawDF.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Summary statistics on a sample of 1000 rows\r\n",
        "display(rawDF.limit(100).summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "# Check the data types of the source data fields\r\n",
        "rawDF.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# This is a great way to test out your transformations.\r\n",
        "# Display just the field with the transformations on a sample of 1000 rows\r\n",
        "from pyspark.sql.functions import *\r\n",
        "display(rawDF.limit(1000).select(substring(col(\"OrderDate\").cast(\"String\"),1,10)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Always import these two sets of libraries at a minimum for spark transformations\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "# Set the date format\r\n",
        "dateFormat = \"yyyy-M-d\"\r\n",
        "\r\n",
        "# Executed the following transformations\r\n",
        "# 1. Select a subset of fields from source\r\n",
        "# 2. Rename fields to more readable names\r\n",
        "# 3. Change data types\r\n",
        "# 4. Concat fields to create the birth date\r\n",
        "# 5. Filter Rows\r\n",
        "cleanDF = rawDF.select(\\\r\n",
        "             #Date and time of the event\r\n",
        "             col(\"CustomerKey\").alias(\"customer_sk\"),\r\n",
        "             col(\"ProductKey\").alias(\"product_sk\"),\r\n",
        "             col(\"OrderDateKey\").alias(\"order_date_sk\"),\r\n",
        "             col(\"ShipDateKey\").alias(\"ship_date_sk\"),\r\n",
        "             col(\"DueDateKey\").alias(\"due_date_sk\"),\r\n",
        "             col(\"PromotionKey\").alias(\"promotion_sk\"),\r\n",
        "             col(\"CurrencyKey\").alias(\"currency_sk\"),\r\n",
        "             col(\"SalesTerritoryKey\").alias(\"sales_territory_sk\"),\r\n",
        "             col(\"SalesOrderNumber\").alias(\"sales_order_no\"),\r\n",
        "             col(\"SalesOrderLineNumber\").alias(\"sales_order_line_no\"),\r\n",
        "             col(\"RevisionNumber\").alias(\"revision_no\"),\r\n",
        "             col(\"OrderQuantity\").alias(\"order_qty\"),\r\n",
        "             #col(\"HouseOwnerFlag\").cast(BooleanType()).alias(\"house_owner_flag\"),\r\n",
        "             to_date(substring(col(\"OrderDate\").cast(\"String\"),1,10),dateFormat).alias(\"order_date\"), \\\r\n",
        "             col(\"SalesAmount\").alias(\"sales_amount\"),\r\n",
        "             col(\"DiscountAmount\").alias(\"discount_amount\"), \\\r\n",
        "             col(\"Freight\").alias(\"freight\") \\\r\n",
        "            ) \\\r\n",
        "            .filter(\"ExtendedAmount > 0\")\r\n",
        "\r\n",
        "display(cleanDF.limit(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "cleanDF.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "cleanDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creating a temporary in memory table helps with further processing using SQL syntax\r\n",
        "# Transforming data using Scala or Python or SQL DOES NOT affect the performance of the processing\r\n",
        "# Ultimately all transformations are optimized by Spark and operated using RDDs\r\n",
        "\r\n",
        "cleanDF.createOrReplaceTempView(\"clean_tmp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3. Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Ideally you want to specify where this database will be stored by using LOCATION parameter\r\n",
        "-- CREATE DATABASE sparklakehouse LOCATION 'abfss://synapse@oneclickpocadls.dfs.core.windows.net/tpcds1tbparquet/'\r\n",
        "CREATE DATABASE IF NOT EXISTS sparklakehouse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#%%sql\r\n",
        "#DROP TABLE sparklakehouse.stg_internet_sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Creating spark tables using delta format allow ACID transactions on data lake tables\r\n",
        "-- This is a one-time task\r\n",
        "CREATE TABLE IF NOT EXISTS sparklakehouse.stg_internet_sales USING DELTA\r\n",
        "AS\r\n",
        "SELECT * FROM clean_tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- We can merge only changes into the target table\r\n",
        "-- This cell should be commented out during the initial load\r\n",
        "MERGE INTO sparklakehouse.stg_internet_sales t\r\n",
        "USING clean_tmp s \r\n",
        "ON t.customer_sk = s.customer_sk \r\n",
        "and t.product_sk = s.product_sk\r\n",
        "and t.order_date_sk = s.order_date_sk\r\n",
        "and t.ship_date_sk = s.ship_date_sk\r\n",
        "and t.due_date_sk = s.due_date_sk\r\n",
        "and t.promotion_sk = s.promotion_sk\r\n",
        "and t.currency_sk = s.currency_sk\r\n",
        "and t.promotion_sk = s.promotion_sk\r\n",
        "and t.sales_order_no = s.sales_order_no\r\n",
        "and t.sales_order_line_no = s.sales_order_line_no\r\n",
        "WHEN MATCHED THEN UPDATE SET *\r\n",
        "WHEN NOT MATCHED THEN INSERT *\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "VACUUM sparklakehouse.stg_internet_sales RETAIN 168 HOURS;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "DESCRIBE DETAIL sparklakehouse.stg_internet_sales"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}